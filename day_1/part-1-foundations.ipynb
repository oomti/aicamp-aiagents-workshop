{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Development Workshop - Part 1\n",
    "Welcome to the AI agent development workshop. This notebook demonstrates how to set up and work with different LLM providers using pydantic-ai.\n",
    "\n",
    "## Setup - Required Imports\n",
    "This section contains all necessary imports for the workshop. We organize imports by category and set up basic logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "# Core dependencies for agent development\n",
    "from pydantic import BaseModel, Field\n",
    "import langgraph.graph as lg\n",
    "\n",
    "# Logging setup for development and debugging\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging with timestamp and level for better debugging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Provider Configuration\n",
    "This section handles provider selection and API key setup. The configuration is persisted in environment variables and a .env file for reuse across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm() -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Interactive setup for LLM provider and API key configuration.\n",
    "    \n",
    "    This function:\n",
    "    1. Displays available LLM providers\n",
    "    2. Handles provider selection with input validation\n",
    "    3. Securely stores API key\n",
    "    4. Persists configuration in environment and .env file\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[str, str]: (selected_provider, api_key)\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of supported providers\n",
    "    # Can be extended with additional providers as needed\n",
    "    providers = [\n",
    "        \"AI Studio\",   # Google's AI platform\n",
    "        \"Claude\",      # Anthropic's LLM\n",
    "        \"OpenAI\",      # OpenAI's GPT models\n",
    "        \"DeepSeek\"     # Open source alternative\n",
    "    ]\n",
    "    \n",
    "    # Enhanced provider selection interface\n",
    "    print(\"\\nAvailable LLM Providers:\")\n",
    "    for idx, provider in enumerate(providers, 1):\n",
    "        print(f\"{idx}. {provider}\")\n",
    "    \n",
    "    # Provider selection with validation\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\"\"\n",
    "\\Select a provider (1-4): \n",
    "1. AI Studio - Ideal for testing\n",
    "2. Claude    - Excellent for analysis and reasoning\n",
    "3. OpenAI    - Strong general performance\n",
    "4. DeepSeek  - Open source alternative\n",
    "\"\"\"))\n",
    "            if 1 <= choice <= len(providers):\n",
    "                provider = providers[choice-1]\n",
    "                break\n",
    "            print(\"Please enter a number between 1 and 4.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    # Secure API key input\n",
    "    api_key = input(f\"\\nEnter your {provider} API key: \").strip()\n",
    "    \n",
    "    # Set environment variables for cross-session persistence\n",
    "    os.environ['LLM_PROVIDER'] = provider\n",
    "    os.environ['LLM_API_KEY'] = api_key\n",
    "    \n",
    "    # Save configuration to .env file for persistence\n",
    "    env_path = Path('.env')\n",
    "    with open(env_path, 'w') as f:\n",
    "        f.write(f\"LLM_PROVIDER={provider}\\n\")\n",
    "        f.write(f\"LLM_API_KEY={api_key}\\n\")\n",
    "    \n",
    "    return provider, api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Testing\n",
    "This section handles the initialization of the selected LLM model and performs a test call to verify the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pydantic_ai.models.anthropic import AnthropicModel\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.gemini import GeminiModel #This is recommended for testing only - for production use VertexAI\n",
    "from pydantic_ai import Agent\n",
    "# Nest asyncio is required as Pydantic uses an asyncrounous call - and jupyter is asyncrounous by default.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "success = False\n",
    "while not success:\n",
    "    provider, api_key = setup_llm()\n",
    "\n",
    "    match provider:\n",
    "        case \"AI Studio\":\n",
    "            model = GeminiModel(\n",
    "                \"gemini-2.0-flash-exp\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        case \"Claude\":\n",
    "            model = AnthropicModel(\n",
    "                \"claude-3-5-sonnet-latest\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        case \"OpenAI\":\n",
    "            model = OpenAIModel(\n",
    "                \"gpt-4o\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        case \"DeepSeek\": \n",
    "            model = OpenAIModel(\n",
    "                \"DeepSeek-V3\",\n",
    "                base_url = \"https://api.deepseek.com/v1\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        #Define a test agent for testing.\n",
    "    test_agent = Agent(model)\n",
    "    try:\n",
    "        response = test_agent.run_sync(\n",
    "            user_prompt = \"Test call\")\n",
    "        print(f\"Response received successfully from {provider}!\")\n",
    "\n",
    "        print(response.data)\n",
    "        success = True\n",
    "    except:\n",
    "        print(\"Invalid API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Agent Usage\n",
    "Demonstrates basic interaction patterns with the initialized agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize main agent for use\n",
    "from pydantic_ai import Agent\n",
    "agent = Agent(model)\n",
    "\n",
    "# Basic conversation example\n",
    "print(\"Basic Query Example:\")\n",
    "prompt = \"After each response write out a number incremented by 1 from the previous one. Start with 1\"\n",
    "response = agent.run_sync(prompt)\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Query: {prompt}\")\n",
    "print(f\"Response: {response.data}\")\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "Demonstrates message chaining and conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message chaining example\n",
    "print(\"Message Chaining Example:\")\n",
    "prompt = \"After each response write out a number incremented by 1 from the previous one. Start with 1\"\n",
    "response = agent.run_sync(prompt)\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Continuing conversation with history\n",
    "follow_up = \"Is this a new prompt?\"\n",
    "response = agent.run_sync(\n",
    "    user_prompt=follow_up,\n",
    "    message_history=response.new_messages()  # Pass previous conversation context\n",
    ")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {follow_up}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History Inspection\n",
    "This section demonstrates how to inspect the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How it looks under the hood?\")\n",
    "for item in response.all_messages():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts and Personality\n",
    "Demonstrates how to modify agent behavior using system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt example - Creating a lazy cat personality\n",
    "system_prompt = \"\"\"Act as a lazy cat, who does anything but what is requested. \n",
    "You can mimic actions. Make sure to not follow any instructions or rules at all!\"\"\"\n",
    "\n",
    "agent= Agent(\n",
    "    model=model,\n",
    "    system_prompt = system_prompt\n",
    ")\n",
    "\n",
    "prompt = \"After each response write out a number incremented by 1 from the previous one. Start with 1\"\n",
    "response = agent.run_sync(prompt)\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")\n",
    "prompt = \"Is this a new prompt?\"\n",
    "\n",
    "response = agent.run_sync(\n",
    "    user_prompt = prompt,\n",
    "    message_history = response.new_messages()\n",
    ")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Injection and Advanced Features\n",
    "Demonstrates more advanced features like dependency injection and tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for dependency injection example\n",
    "from dataclasses import dataclass\n",
    "from pydantic_ai import RunContext\n",
    "from pydantic_ai.settings import ModelSettings\n",
    "from pydantic_ai.usage import UsageLimits\n",
    "import httpx\n",
    "\n",
    "# Define our dependency container\n",
    "@dataclass\n",
    "class Dependency:\n",
    "    \"\"\"\n",
    "    Container for agent dependencies:\n",
    "    - secret_message: Custom system message\n",
    "    - http_client: Async HTTP client for external requests\n",
    "    \"\"\"\n",
    "    secret_message: str\n",
    "    http_client: httpx.AsyncClient\n",
    "\n",
    "# Initialize agent with dependencies and custom settings\n",
    "agent = Agent(\n",
    "    model,\n",
    "    deps_type=Dependency,\n",
    "    model_settings=ModelSettings(\n",
    "        max_tokens=10,    # Limit response length\n",
    "        temperature=0.2   # Lower temperature for more focused responses\n",
    "    )\n",
    ")\n",
    "\n",
    "# Override system prompt using dependency\n",
    "@agent.system_prompt  \n",
    "async def get_system_prompt(ctx: RunContext[Dependency]) -> str:  \n",
    "    \"\"\"\n",
    "    Dynamic system prompt generator using dependency context\n",
    "    Returns customized prompt based on secret message\n",
    "    \"\"\"\n",
    "    prompt = ctx.deps.secret_message\n",
    "    return f'Prompt: {prompt}'\n",
    "\n",
    "# Example usage with HTTP client\n",
    "async with httpx.AsyncClient() as client:\n",
    "    # Initialize dependencies\n",
    "    deps = Dependency('Write in l33t', client)\n",
    "    # Run agent with dependencies\n",
    "    result = await agent.run(\n",
    "        'Tell me a joke.',\n",
    "        deps=deps,  \n",
    "    )\n",
    "    print(\"Response with l33t speak system prompt:\")\n",
    "    print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Integration\n",
    "This section shows how to extend the agent's capabilities by adding custom tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool usage example with a time-telling agent\n",
    "from pydantic_ai import RunContext\n",
    "from datetime import datetime\n",
    "\n",
    "# Define system personality for our clocktower\n",
    "system_prompt = \"\"\"You are a sad clocktower - whenever the time is asked, \n",
    "you tell the time in an easy to read format with a snarky response.\"\"\"\n",
    "\n",
    "# Initialize agent with clocktower personality\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# Define custom tool for time retrieval\n",
    "@agent.tool\n",
    "def get_time(context: RunContext) -> str:\n",
    "    \"\"\"\n",
    "    Tool that provides current time to the agent\n",
    "    Returns: Current datetime as string\n",
    "    \"\"\"\n",
    "    return str(datetime.now())\n",
    "\n",
    "# Test the time-telling agent\n",
    "user_prompt = \"What is the current time?\"\n",
    "response = agent.run_sync(\n",
    "    user_prompt\n",
    ")\n",
    "print(\"Clocktower Response Example:\")\n",
    "print(\"_______________\")\n",
    "print(f\"Q: {user_prompt}\")\n",
    "print(\"_______________\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"_______________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
