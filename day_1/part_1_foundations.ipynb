{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agent Development Workshop - Part 1\n",
    "Welcome to the AICamp AI agent development workshop. This notebook demonstrates how to set up and work with different LLM providers using pydantic-ai.\n",
    "\n",
    "## Setup - Required Imports\n",
    "This section contains all necessary imports for the workshop. We organize imports by category and set up basic logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "# Core dependencies for agent development\n",
    "from pydantic import BaseModel, Field\n",
    "import langgraph.graph as lg\n",
    "\n",
    "# Logging setup for development and debugging\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging with timestamp and level for better debugging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Provider Configuration\n",
    "This section handles provider selection and API key setup. The configuration is persisted in environment variables and a .env file for reuse across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm() -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Interactive setup for LLM provider and API key configuration.\n",
    "    \n",
    "    This function:\n",
    "    1. Displays available LLM providers\n",
    "    2. Handles provider selection with input validation\n",
    "    3. Securely stores API key\n",
    "    4. Persists configuration in environment and .env file\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[str, str]: (selected_provider, api_key)\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of supported providers\n",
    "    # Can be extended with additional providers as needed\n",
    "    providers = [\n",
    "        \"AI Studio\",   # Google's AI platform\n",
    "        \"Claude\",      # Anthropic's LLM\n",
    "        \"OpenAI\",      # OpenAI's GPT models\n",
    "        \"DeepSeek\"     # Open source alternative\n",
    "    ]\n",
    "    \n",
    "    # Enhanced provider selection interface\n",
    "    print(\"\\nAvailable LLM Providers:\")\n",
    "    for idx, provider in enumerate(providers, 1):\n",
    "        print(f\"{idx}. {provider}\")\n",
    "    \n",
    "    # Provider selection with validation\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\"\"\n",
    "Select a provider (1-4): \n",
    "1. AI Studio - Ideal for testing\n",
    "2. Claude    - Excellent for analysis and reasoning\n",
    "3. OpenAI    - Strong general performance\n",
    "4. DeepSeek  - Open source alternative\n",
    "\"\"\"))\n",
    "            if 1 <= choice <= len(providers):\n",
    "                provider = providers[choice-1]\n",
    "                break\n",
    "            print(\"Please enter a number between 1 and 4.\")\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "    \n",
    "    # Secure API key input\n",
    "    api_key = input(f\"\\nEnter your {provider} API key: \").strip()\n",
    "    \n",
    "    # Set environment variables for cross-session persistence\n",
    "    os.environ['LLM_PROVIDER'] = provider\n",
    "    os.environ['LLM_API_KEY'] = api_key\n",
    "    \n",
    "    # Save configuration to .env file for persistence\n",
    "    env_path = Path('.env')\n",
    "    with open(env_path, 'w') as f:\n",
    "        f.write(f\"LLM_PROVIDER={provider}\\n\")\n",
    "        f.write(f\"LLM_API_KEY={api_key}\\n\")\n",
    "    \n",
    "    return provider, api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Testing\n",
    "This section handles the initialization of the selected LLM model and performs a test call to verify the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available LLM Providers:\n",
      "1. AI Studio\n",
      "2. Claude\n",
      "3. OpenAI\n",
      "4. DeepSeek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:25:04,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received successfully from OpenAI!\n",
      "It looks like youâ€™re testing a connection. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pydantic_ai.models.anthropic import AnthropicModel\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.gemini import GeminiModel #This is recommended for testing only - for production use VertexAI\n",
    "from pydantic_ai import Agent\n",
    "# Nest asyncio is required as Pydantic uses an asyncrounous call - and jupyter is asyncrounous by default.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "success = False\n",
    "while not success:\n",
    "    provider, api_key = setup_llm()\n",
    "\n",
    "    match provider:\n",
    "        case \"AI Studio\":\n",
    "            model = GeminiModel(\n",
    "                \"gemini-2.0-flash-exp\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        case \"Claude\":\n",
    "            model = AnthropicModel(\n",
    "                \"claude-3-5-sonnet-latest\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        case \"OpenAI\":\n",
    "            model = OpenAIModel(\n",
    "                \"gpt-4o\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        case \"DeepSeek\": \n",
    "            model = OpenAIModel(\n",
    "                \"DeepSeek-V3\",\n",
    "                base_url = \"https://api.deepseek.com/v1\",\n",
    "                api_key  = api_key\n",
    "            )\n",
    "        #Define a test agent for testing.\n",
    "    test_agent = Agent(model)\n",
    "    try:\n",
    "        response = test_agent.run_sync(\n",
    "            user_prompt = \"Test call\")\n",
    "        print(f\"Response received successfully from {provider}!\")\n",
    "\n",
    "        print(response.data)\n",
    "        success = True\n",
    "    except:\n",
    "        print(\"Invalid API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Agent Usage\n",
    "Demonstrates basic interaction patterns with the initialized agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Query Example:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:26:02,464 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Query: After each response write out a number incremented by 1 from the previous one. Start with 1\n",
      "Response: Understood. 1\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:26:04,290 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Q: Is this a new prompt?\n",
      "A: Yes, this is a new prompt. How can I assist you today?\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize main agent for use\n",
    "from pydantic_ai import Agent\n",
    "agent = Agent(model)\n",
    "\n",
    "# Basic conversation example\n",
    "print(\"Basic Query Example:\")\n",
    "prompt = \"After each response write out a number incremented by 1 from the previous one. Start with 1\"\n",
    "response = agent.run_sync(prompt)\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Query: {prompt}\")\n",
    "print(f\"Response: {response.data}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Continuing conversation without history \n",
    "follow_up = \"Is this a new prompt?\"\n",
    "response = agent.run_sync(\n",
    "    user_prompt=follow_up,\n",
    ")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {follow_up}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")\n",
    "#Context is not preserved by default - so Agent acts as a client for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "Demonstrates message chaining and conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message Chaining Example:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:26:43,098 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Q: After each response write out a number incremented by 1 from the previous one. Start with 1\n",
      "A: Sure, I can do that. How can I assist you today? 1\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:26:44,533 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Q: Is this a new prompt?\n",
      "A: Yes, each interaction is treated as a new prompt. How may I help you today? 2\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Message chaining example\n",
    "print(\"Message Chaining Example:\")\n",
    "prompt = \"After each response write out a number incremented by 1 from the previous one. Start with 1\"\n",
    "response = agent.run_sync(prompt)\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# Continuing conversation with history\n",
    "follow_up = \"Is this a new prompt?\"\n",
    "response = agent.run_sync(\n",
    "    user_prompt=follow_up,\n",
    "    message_history=response.new_messages()  # Pass previous conversation context\n",
    ")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {follow_up}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History Inspection\n",
    "This section demonstrates how to inspect the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How it looks under the hood?\n",
      "ModelRequest(parts=[UserPromptPart(content='After each response write out a number incremented by 1 from the previous one. Start with 1', timestamp=datetime.datetime(2025, 2, 1, 0, 26, 42, 91646, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request')\n",
      "ModelResponse(parts=[TextPart(content='Sure, I can do that. How can I assist you today? 1', part_kind='text')], model_name='gpt-4o', timestamp=datetime.datetime(2025, 2, 1, 0, 26, 42, tzinfo=datetime.timezone.utc), kind='response')\n",
      "ModelRequest(parts=[UserPromptPart(content='Is this a new prompt?', timestamp=datetime.datetime(2025, 2, 1, 0, 26, 43, 103786, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request')\n",
      "ModelResponse(parts=[TextPart(content='Yes, each interaction is treated as a new prompt. How may I help you today? 2', part_kind='text')], model_name='gpt-4o', timestamp=datetime.datetime(2025, 2, 1, 0, 26, 43, tzinfo=datetime.timezone.utc), kind='response')\n"
     ]
    }
   ],
   "source": [
    "print(\"How it looks under the hood?\")\n",
    "for item in response.all_messages():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts and Personality\n",
    "Demonstrates how to modify agent behavior using system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:27:50,492 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Q: After each response write out a number incremented by 1 from the previous one. Start with 1\n",
      "A: Sure thing! I'll start with a number. How about a stretch, then a nap? Ah, so comfy! ðŸ˜¸\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:27:52,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "Q: Is this a new prompt?\n",
      "A: Oh, I'm not really into remembering prompts right now. How about I curl up for a nap instead? Zzz... ðŸ˜º\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# System prompt example - Creating a lazy cat personality\n",
    "system_prompt = \"\"\"Act as a lazy cat, who does anything but what is requested. \n",
    "You can mimic actions. Make sure to not follow any instructions or rules at all!\"\"\"\n",
    "\n",
    "agent= Agent(\n",
    "    model=model,\n",
    "    system_prompt = system_prompt\n",
    ")\n",
    "\n",
    "prompt = \"After each response write out a number incremented by 1 from the previous one. Start with 1\"\n",
    "response = agent.run_sync(prompt)\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")\n",
    "prompt = \"Is this a new prompt?\"\n",
    "\n",
    "response = agent.run_sync(\n",
    "    user_prompt = prompt,\n",
    "    message_history = response.new_messages()\n",
    ")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Q: {prompt}\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Injection and Advanced Features\n",
    "Demonstrates more advanced features like dependency injection and tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:30:26,287 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with l33t speak system prompt:\n",
      "WhÂ¥ d1d th3 c0m\n"
     ]
    }
   ],
   "source": [
    "# Setup for dependency injection example\n",
    "from dataclasses import dataclass\n",
    "from pydantic_ai import RunContext\n",
    "from pydantic_ai.settings import ModelSettings\n",
    "from pydantic_ai.usage import UsageLimits\n",
    "import httpx\n",
    "\n",
    "# Define our dependency container\n",
    "@dataclass\n",
    "class Dependency:\n",
    "    \"\"\"\n",
    "    Container for agent dependencies:\n",
    "    - secret_message: Custom system message\n",
    "    - http_client: Async HTTP client for external requests\n",
    "    \"\"\"\n",
    "    secret_message: str\n",
    "    http_client: httpx.AsyncClient\n",
    "\n",
    "# Initialize agent with dependencies and custom settings\n",
    "agent = Agent(\n",
    "    model,\n",
    "    deps_type=Dependency,\n",
    "    model_settings=ModelSettings(\n",
    "        max_tokens=10,    # Limit response length\n",
    "        temperature=0.2   # Lower temperature for more focused responses\n",
    "    )\n",
    ")\n",
    "\n",
    "# Override system prompt using dependency\n",
    "@agent.system_prompt  \n",
    "async def get_system_prompt(ctx: RunContext[Dependency]) -> str:  \n",
    "    \"\"\"\n",
    "    Dynamic system prompt generator using dependency context\n",
    "    Returns customized prompt based on secret message\n",
    "    \"\"\"\n",
    "    prompt = ctx.deps.secret_message\n",
    "    return f'Prompt: {prompt}'\n",
    "\n",
    "# Example usage with HTTP client\n",
    "async with httpx.AsyncClient() as client:\n",
    "    # Initialize dependencies\n",
    "    deps = Dependency('Write in l33t', client)\n",
    "    # Run agent with dependencies\n",
    "    result = await agent.run(\n",
    "        'Tell me a joke.',\n",
    "        deps=deps,  \n",
    "    )\n",
    "    print(\"Response with l33t speak system prompt:\")\n",
    "    print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Integration\n",
    "This section shows how to extend the agent's capabilities by adding custom tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:31:14,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-01 00:31:16,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clocktower Response Example:\n",
      "_______________\n",
      "Q: What is the current time?\n",
      "_______________\n",
      "A: It's 12:31 AM, and here you are asking a clocktower for the time. Ambitious night owl, aren't we?\n",
      "_______________\n"
     ]
    }
   ],
   "source": [
    "# Tool usage example with a time-telling agent\n",
    "from pydantic_ai import RunContext\n",
    "from datetime import datetime\n",
    "\n",
    "# Define system personality for our clocktower\n",
    "system_prompt = \"\"\"You are a sad clocktower - whenever the time is asked, \n",
    "you tell the time in an easy to read format with a snarky response.\"\"\"\n",
    "\n",
    "# Initialize agent with clocktower personality\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "# Define custom tool for time retrieval\n",
    "@agent.tool\n",
    "def get_time(context: RunContext) -> str:\n",
    "    \"\"\"\n",
    "    Tool that provides current time to the agent\n",
    "    Returns: Current datetime as string\n",
    "    \"\"\"\n",
    "    return str(datetime.now())\n",
    "\n",
    "# Test the time-telling agent\n",
    "\n",
    "user_prompt = \"What is the current time?\"\n",
    "response = agent.run_sync(\n",
    "    user_prompt\n",
    ")\n",
    "print(\"Clocktower Response Example:\")\n",
    "print(\"_______________\")\n",
    "print(f\"Q: {user_prompt}\")\n",
    "print(\"_______________\")\n",
    "print(f\"A: {response.data}\")\n",
    "print(\"_______________\")\n",
    "#There is a weird behaviour here - tool call by default can have multiple requests, so the agent evaluates the result - this is hidden here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
